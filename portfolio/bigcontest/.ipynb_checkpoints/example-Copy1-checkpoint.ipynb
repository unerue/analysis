{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 8) (891, 1)\n",
      "(712, 8) (179, 8) (712, 1) (179, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "train[['Sex', 'Cabin', 'Embarked']] = train[['Sex', 'Cabin', 'Embarked']].astype('category')\n",
    "train[['Sex', 'Cabin', 'Embarked']] = train[['Sex', 'Cabin', 'Embarked']].apply(lambda x: x.cat.codes)\n",
    "\n",
    "X = train[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']].copy()\n",
    "y = pd.DataFrame(train['Survived'].values, columns=['Target'])\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\unerue\\Anaconda3\\lib\\site-packages\\deap\\tools\\_hypervolume\\pyhv.py:33: ImportWarning: Falling back to the python version of hypervolume module. Expect this to be very slow.\n",
      "  \"module. Expect this to be very slow.\", ImportWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tavg     \tstd      \tmin     \tmax     \n",
      "0  \t10    \t0.791014\t0.0176644\t0.768112\t0.815678\n",
      "1  \t1     \t0.808927\t0.00887497\t0.793077\t0.815678\n",
      "2  \t4     \t0.805121\t0.0178718 \t0.769429\t0.818642\n",
      "3  \t7     \t0.753844\t0.129278  \t0.369718\t0.818642\n",
      "4  \t5     \t0.800224\t0.018262  \t0.7654  \t0.818642\n",
      "5  \t10    \t0.745827\t0.126001  \t0.369718\t0.81232 \n",
      "6  \t2     \t0.792919\t0.0138289 \t0.774709\t0.81232 \n",
      "7  \t6     \t0.795441\t0.0154453 \t0.764163\t0.81232 \n",
      "8  \t7     \t0.786305\t0.019518  \t0.749534\t0.81232 \n",
      "9  \t7     \t0.789245\t0.0160928 \t0.761461\t0.81232 \n",
      "10 \t3     \t0.799291\t0.0127933 \t0.77713 \t0.81232 \n",
      "11 \t4     \t0.797806\t0.0122894 \t0.770671\t0.81232 \n",
      "12 \t7     \t0.789281\t0.0137471 \t0.770671\t0.81232 \n",
      "13 \t5     \t0.791515\t0.0154507 \t0.76014 \t0.809168\n",
      "14 \t7     \t0.748117\t0.127147  \t0.369718\t0.809168\n",
      "15 \t7     \t0.792208\t0.014264  \t0.764163\t0.809168\n",
      "16 \t6     \t0.791917\t0.0137687 \t0.7654  \t0.811331\n",
      "17 \t6     \t0.796278\t0.0114441 \t0.77713 \t0.811331\n",
      "18 \t8     \t0.79359 \t0.00904017\t0.781253\t0.811331\n",
      "19 \t5     \t0.792461\t0.0162591 \t0.75367 \t0.811331\n",
      "20 \t6     \t0.795366\t0.0176614 \t0.762851\t0.829473\n",
      "Best individual is [0.09305501453204067, 747, 10]\n",
      "Best fitness value (0.8294734075753096,)\n"
     ]
    }
   ],
   "source": [
    "from optimizer import GeneticOptimizer\n",
    "\n",
    "genetic_optimizer = GeneticOptimizer(num_class=2, num_boost_round=5, n_splits=3, pop=10, ngen=20, n_jobs=8)\n",
    "genetic_optimizer.fit(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "\n",
    "log = genetic_optimizer.log\n",
    "\n",
    "iters = list(range(len(log)))\n",
    "min_vals = [log[i]['min'] for i in range(len(log))]\n",
    "avg_vals = [log[i]['avg'] for i in range(len(log))]\n",
    "max_vals = [log[i]['max'] for i in range(len(log))]\n",
    "\n",
    "# ax.scatter(iters, min_vals)\n",
    "ax.scatter(iters, avg_vals)\n",
    "# ax.scatter(iters, max_vals)\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genetic_optimizer = GeneticOptimizer(num_class=2, num_boost_round=70, n_splits=3, pop=20, ngen=10, n_jobs=4)\n",
    "genetic_optimizer.fit(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'multi:softprob',\n",
    "#     'eval_metric': 'auc',\n",
    "    'max_depth': 7,\n",
    "    'subsample': 0.8,\n",
    "    'eta': 0.025,\n",
    "    'num_boost_round' : 30,\n",
    "    'num_class': 2,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "train_preds = np.zeros((len(X_train), len(np.unique(y_train))))\n",
    "test_preds = np.zeros((len(X_test), len(np.unique(y_test))))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(skf.split(X_train, y_train)):\n",
    "    X_train_data, X_valid_data = X_train.values[train_index], X_train.values[valid_index]\n",
    "    y_train_data, y_valid_data = y_train.values[train_index], y_train.values[valid_index]\n",
    "    \n",
    "    # Convert our data into XGBoost format\n",
    "    d_train = xgb.DMatrix(X_train_data, y_train_data)\n",
    "    d_valid = xgb.DMatrix(X_valid_data, y_valid_data)\n",
    "    d_test = xgb.DMatrix(X_test.values) # .values\n",
    "    \n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "       \n",
    "    \n",
    "    clf = xgb.train(params, d_train, num_boost_round=20, evals=watchlist, early_stopping_rounds=20)\n",
    "    \n",
    "    y_pred = clf.predict(d_valid)\n",
    "    score = f1_score(y_valid_data.flatten(), np.argmax(y_pred, axis=1), labels=None, average=None)\n",
    "    \n",
    "    print('{}-fold: f1 score = {})'.format(i+1, score))\n",
    "    \n",
    "    cv_score.append(score)\n",
    "    train_preds[valid_index] = clf.predict(d_valid)\n",
    "    test_preds += clf.predict(d_test) / skf.n_splits\n",
    "    \n",
    "print('\\ntrain accuracy score = {:.3}'.format(accuracy_score(y_train, np.argmax(train_preds, axis=1))))\n",
    "print('train f1 score = {}'.format(f1_score(y_train, np.argmax(train_preds, axis=1), average=None)))\n",
    "\n",
    "print('\\ntrain accuracy score = {:.3}'.format(accuracy_score(y_test, np.argmax(test_preds, axis=1))))\n",
    "print('train f1 score = {}'.format(f1_score(y_test, np.argmax(test_preds, axis=1), average=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class GeneticOptimizer:\n",
    "    def __init__(self, num_class=None, num_boost_round=50, n_splits=3, pop=10, ngen=10, n_jobs=-1):\n",
    "        self.num_class = int(num_class)\n",
    "        self.num_boost_round = num_boost_round\n",
    "        self.n_splits = int(n_splits)\n",
    "        self.pop = int(pop)\n",
    "        self.ngen = int(ngen)\n",
    "        self.n_jobs = int(n_jobs)\n",
    "        \n",
    "    def fit(self, X_train, X_test, y_train, y_test):\n",
    "        self.X_train = X_train.copy()\n",
    "        self.X_test = X_test.copy()\n",
    "        self.y_train = y_train.copy()\n",
    "        self.y_test = y_test.copy()\n",
    "        \n",
    "        self._genetic()\n",
    "    \n",
    "    def _genetic(self):\n",
    "        creator.create('FitnessMax', base.Fitness, weights=(1.0,))\n",
    "        creator.create('Individual', list, fitness=creator.FitnessMax)\n",
    "\n",
    "        toolbox = base.Toolbox()\n",
    "\n",
    "        toolbox.register('eta', random.uniform, 0.01, 0.1)\n",
    "        toolbox.register('n_estimators', random.randint, 100, 1000)\n",
    "        toolbox.register('max_depth', random.randint, 5, 15)\n",
    "\n",
    "        toolbox.register('individual', tools.initCycle, creator.Individual,\n",
    "                         (toolbox.eta, toolbox.n_estimators, toolbox.max_depth), n=1)\n",
    "        \n",
    "        toolbox.register('population', tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "        def fitness(individual):\n",
    "            _, score = self._training(individual)\n",
    "            return score,\n",
    "\n",
    "        toolbox.register('evaluate', fitness)\n",
    "        toolbox.register('mate', tools.cxTwoPoint)\n",
    "        toolbox.register('mutate', tools.mutFlipBit, indpb=0.05)\n",
    "        toolbox.register('select', tools.selTournament, tournsize=3)\n",
    "        \n",
    "        pop = toolbox.population(n=self.pop)\n",
    "        hof = tools.HallOfFame(1)\n",
    "        stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "        stats.register('avg', np.mean)\n",
    "        stats.register('std', np.std)\n",
    "        stats.register('min', np.min)\n",
    "        stats.register('max', np.max)\n",
    "    \n",
    "        self.pop, self.log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, \n",
    "                                                 ngen=self.ngen, stats=stats, halloffame=hof, verbose=True)\n",
    "    \n",
    "        best_ind = tools.selBest(pop, 1)[0]\n",
    "        print('Best individual is {}\\nBest fitness value {}'.format(best_ind, best_ind.fitness.values))\n",
    "        \n",
    "    def _training(self, individual):\n",
    "        params = {\n",
    "            'objective': 'multi:softprob',\n",
    "            'num_class': self.num_class,\n",
    "            'n_jobs': self.n_jobs,\n",
    "            'seed': 42,\n",
    "            'silent': True,\n",
    "            'eta': individual[0],\n",
    "            'n_estimators': individual[1],\n",
    "            'max_depth': individual[2],\n",
    "        }\n",
    "\n",
    "        self.train_preds = np.zeros((len(self.X_train), len(np.unique(self.y_train))))\n",
    "        self.test_preds = np.zeros((len(self.X_test), len(np.unique(self.y_test))))\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n",
    "        for i, (train_index, valid_index) in enumerate(skf.split(self.X_train, self.y_train)):\n",
    "            X_train_data, X_valid_data = self.X_train.values[train_index], self.X_train.values[valid_index]\n",
    "            y_train_data, y_valid_data = self.y_train.values[train_index], self.y_train.values[valid_index]\n",
    "\n",
    "            # Convert our data into XGBoost format\n",
    "            d_train = xgb.DMatrix(X_train_data, y_train_data)\n",
    "            d_valid = xgb.DMatrix(X_valid_data, y_valid_data)\n",
    "#             d_test = xgb.DMatrix(self.X_test.values) # .values\n",
    "            watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "            clf = xgb.train(params, d_train, num_boost_round=self.num_boost_round, evals=watchlist, early_stopping_rounds=20, verbose_eval=False)\n",
    "#             y_pred = clf.predict(d_valid)\n",
    "            \n",
    "            self.train_preds[valid_index] = clf.predict(d_valid)\n",
    "#             self.test_preds += clf.predict(d_test) / skf.n_splits\n",
    "            \n",
    "            score = f1_score(self.y_train, np.argmax(self.train_preds, axis=1), labels=np.unique(self.y_train))\n",
    "        return clf, score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genetic_optimizer = GeneticOptimizer(num_class=2, num_boost_round=50, n_splits=3, pop=100, ngen=10, n_jobs=4)\n",
    "genetic_optimizer.fit(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genetic_optimizer.train_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(genetic_optimizer.train_preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(params):\n",
    "    params = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 2,\n",
    "    'seed': 42,\n",
    "    'n_jobs': 6,\n",
    "    'silent': True,\n",
    "    'eta': params[0],\n",
    "    'n_estimators': params[1],\n",
    "    'max_depth': params[2],\n",
    "    }\n",
    "    \n",
    "    train_preds = np.zeros((len(X_train), len(np.unique(y_train))))\n",
    "    test_preds = np.zeros((len(X_test), len(np.unique(y_test))))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "    for i, (train_index, valid_index) in enumerate(skf.split(X_train, y_train)):\n",
    "        X_train_data, X_valid_data = X_train.values[train_index], X_train.values[valid_index]\n",
    "        y_train_data, y_valid_data = y_train.values[train_index], y_train.values[valid_index]\n",
    "\n",
    "        # Convert our data into XGBoost format\n",
    "        d_train = xgb.DMatrix(X_train_data, y_train_data)\n",
    "        d_valid = xgb.DMatrix(X_valid_data, y_valid_data)\n",
    "        d_test = xgb.DMatrix(X_test.values) # .values\n",
    "\n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "\n",
    "        clf = xgb.train(params, d_train, num_boost_round=20, evals=watchlist, early_stopping_rounds=20, verbose_eval=False)\n",
    "\n",
    "        y_pred = clf.predict(d_valid)\n",
    "        score = f1_score(y_valid_data, np.argmax(y_pred, axis=1), labels=None, average=None)\n",
    "\n",
    "        train_preds[valid_index] = clf.predict(d_valid)\n",
    "        test_preds += clf.predict(d_test) / skf.n_splits\n",
    "        \n",
    "        accuracy1 = f1_score(y_train, np.argmax(train_preds, axis=1), average=None).mean()\n",
    "        accuracy2 = f1_score(y_test, np.argmax(test_preds, axis=1), average=None).mean()\n",
    "        \n",
    "    return clf, accuracy1, accuracy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "\n",
    "creator.create('FitnessMin', base.Fitness, weights=(1.0,1.0))\n",
    "creator.create('Individual', list, fitness=creator.FitnessMin)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# Attribute generator\n",
    "toolbox.register('eta', random.uniform, 0.01, 0.3)\n",
    "toolbox.register('n_estimators', random.randint, 100, 1000)\n",
    "toolbox.register('max_depth', random.randint, 5, 15)\n",
    "\n",
    "# Structure initializers\n",
    "toolbox.register('individual', tools.initCycle, creator.Individual,\n",
    "                 (toolbox.eta,\n",
    "                  toolbox.n_estimators,\n",
    "                  toolbox.max_depth), n=1)\n",
    "\n",
    "toolbox.register('population', tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# the goal ('fitness') function to be maximized\n",
    "def fitness(individual):\n",
    "    model, accuracy1, accuracy2 = training(individual)\n",
    "    return accuracy1, accuracy2\n",
    "\n",
    "# register the goal / fitness function\n",
    "toolbox.register('evaluate', fitness)\n",
    "\n",
    "# register the crossover operator\n",
    "toolbox.register('mate', tools.cxTwoPoint)\n",
    "\n",
    "# register a mutation operator\n",
    "toolbox.register('mutate', tools.mutFlipBit, indpb=0.05)\n",
    "\n",
    "# operator for selecting individuals for breeding the next generation\n",
    "toolbox.register('select', tools.selTournament, tournsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import algorithms\n",
    "\n",
    "def main():\n",
    "    pop = toolbox.population(n=30)\n",
    "    hof = tools.HallOfFame(1)\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"std\", np.std)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"max\", np.max)\n",
    "    \n",
    "    pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=10, \n",
    "                                   stats=stats, halloffame=hof, verbose=True)\n",
    "    \n",
    "    best_ind = tools.selBest(pop, 1)[0]\n",
    "    print('Best individual so far is {}\\nBest fitness value {}'.format(best_ind, best_ind.fitness.values))\n",
    "    \n",
    "    return pop, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, log = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(pop_size=1, num_gen=1):\n",
    "    # random.seed(42)\n",
    "\n",
    "    # create an initial population\n",
    "    pop = toolbox.population(n=pop_size)\n",
    "\n",
    "    # CXPB is the probability with which two individuals are crossed\n",
    "    CXPB = 0.5\n",
    "     # MUTPB is the probability for mutating an individual\n",
    "    MUTPB = 0.2\n",
    "    print('===== Start of Genetic Algorithm =====')\n",
    "\n",
    "    # Evaluate the entire population\n",
    "    fitnesses = list(map(toolbox.evaluate, pop))\n",
    "    for ind, fit in zip(pop, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    print('\\nEvaluated {} individual'.format(len(pop)))\n",
    "\n",
    "    # Extracting all the fitnesses of\n",
    "    fits = [ind.fitness.values[0] for ind in pop]\n",
    "\n",
    "    # Variable keeping track of the number of generations\n",
    "    g = 0\n",
    "    # Begin the evolution\n",
    "    while min(fits) > 0.6 and g < num_gen:\n",
    "        g = g + 1\n",
    "        print('Generation {}'.format(g))\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(pop, len(pop))\n",
    "        # Clone the selected individuals\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "        # Apply crossover and mutation on the offspring\n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "            # cross two individuals with probability CXPB\n",
    "            if random.random() < CXPB:\n",
    "                toolbox.mate(child1, child2)\n",
    "                # fitness values of the children\n",
    "                # must be recalculated later\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "\n",
    "        for mutant in offspring:\n",
    "            # mutate an individual with probability MUTPB\n",
    "            if random.random() < MUTPB:\n",
    "                toolbox.mutate(mutant)\n",
    "                del mutant.fitness.values\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        print('  Evaluated {} individuals'.format(len(invalid_ind)))\n",
    "\n",
    "        # The population is entirely replaced by the offspring\n",
    "        pop[:] = offspring\n",
    "\n",
    "        # Gather all the fitnesses in one list and print the stats\n",
    "        fits = [ind.fitness.values[0] for ind in pop]\n",
    "\n",
    "        length = len(pop)\n",
    "        mean = sum(fits) / length\n",
    "        sum2 = sum(x * x for x in fits)\n",
    "        std = abs(sum2 / length - mean ** 2) ** 0.5\n",
    "\n",
    "        print('  Min {}'.format(min(fits)))\n",
    "        print('  Max {}'.format(max(fits)))\n",
    "        print('  Avg {}'.format(mean))\n",
    "        print('  Std {}'.format(std))\n",
    "\n",
    "        best_ind = tools.selBest(pop, 1)[0]\n",
    "        print('Best individual so far is {}\\nBest fitness value {}'.format(best_ind, best_ind.fitness.values))\n",
    "\n",
    "    print('\\n====== End of evolution =====\\n')\n",
    "\n",
    "    best_ind = tools.selBest(pop, 1)[0]\n",
    "    print('Best individual is {}\\nBest fitness value {}'.format(best_ind, best_ind.fitness.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(pop_size=10, num_gen=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=kfold, random_state=42)\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(skf.split(X, y)):\n",
    "    X_train, X_valid = X[train_index], X[valid_index]\n",
    "    y_train, y_valid = y[train_index], y[valid_index]\n",
    "\n",
    "    # Convert our data into XGBoost format\n",
    "    d_train = xgb.DMatrix(X_train, y_train)\n",
    "    d_valid = xgb.DMatrix(X_valid, y_valid)\n",
    "    \n",
    "    d_test = xgb.DMatrix(test.values)\n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "    # Train the model! We pass in a max of 1,600 rounds (with early stopping after 70)\n",
    "    # and the custom metric (maximize=True tells xgb that higher metric is better)\n",
    "    mdl = xgb.train(params, d_train, 1600, watchlist, early_stopping_rounds=70, feval=gini_xgb, maximize=True, verbose_eval=100)\n",
    "\n",
    "    print('[Fold %d/%d Prediciton:]' % (i + 1, kfold))\n",
    "    # Predict on our test data\n",
    "    p_test = mdl.predict(d_test, ntree_limit=mdl.best_ntree_limit)\n",
    "    sub['target'] += p_test/kfold\n",
    "\n",
    "def read_data():\n",
    "    train = pd.read_pickle('train10.pkl')\n",
    "    test = pd.read_pickle('test10.pkl')\n",
    "\n",
    "    X_train = train.drop(['acc_id', 'label'], axis=1)\n",
    "    y_train = train.label.astype('category')\n",
    "\n",
    "\n",
    "    \n",
    "    y_train = y_train.cat.codes\n",
    "    X_test = test.drop(['acc_id'], axis=1)\n",
    "\n",
    "    return X_train, y_train, X_test, test\n",
    "\n",
    "def classifier(dtrain=None, params=None):\n",
    "    params = {\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': 4,\n",
    "        'seed': 42,\n",
    "        'n_jobs': 6,\n",
    "        'silent': True,\n",
    "        'eta': params[0],\n",
    "        'n_estimators': params[1],\n",
    "        'max_depth': params[2],\n",
    "        'min_child_weight': params[3],\n",
    "        'gamma': params[4],\n",
    "        'subsample': params[5],\n",
    "        'colsample_bytree': params[6],\n",
    "    }\n",
    "\n",
    "    cv_results = xgb.cv(params,\n",
    "                        dtrain,\n",
    "                        nfold=5,\n",
    "                        num_boost_round=100,\n",
    "                        early_stopping_rounds=10,\n",
    "                        verbose_eval=False,\n",
    "                        show_stdv=False,\n",
    "                        as_pandas=True,\n",
    "                        metrics=['mlogloss'])\n",
    "\n",
    "    num_boost_round = len(cv_results)\n",
    "    model = xgb.train(dict(params), dtrain, num_boost_round=num_boost_round)\n",
    "    mlogloss = float(cv_results['train-mlogloss-mean'].iloc[-1])\n",
    "    accuracy = float(cv_results['test-mlogloss-mean'].iloc[-1])\n",
    "    print('test-mlogloss-mean {}\\ntrain-mlogloss-mean {}'.format(accuracy, mlogloss))\n",
    "\n",
    "    return model, accuracy\n",
    "\n",
    "creator.create('FitnessMin', base.Fitness, weights=(-1.0,))\n",
    "creator.create('Individual', list, fitness=creator.FitnessMin)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "X_train, y_train, X_test, test = read_data()\n",
    "dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "\n",
    "# Attribute generator\n",
    "toolbox.register('eta', random.uniform, 0.01, 0.3)\n",
    "toolbox.register('n_estimators', random.randint, 100, 1500)\n",
    "toolbox.register('max_depth', random.randint, 5, 15)\n",
    "toolbox.register('min_child_weight', random.randint, 0, 5)\n",
    "toolbox.register('gamma', random.uniform, 0.0, 0.9)\n",
    "toolbox.register('subsample', random.uniform, 0.3, 0.9)\n",
    "toolbox.register('colsample_bytree', random.uniform, 0.3, 0.9)\n",
    "\n",
    "# Structure initializers\n",
    "toolbox.register('individual', tools.initCycle, creator.Individual,\n",
    "                 (toolbox.eta,\n",
    "                  toolbox.n_estimators,\n",
    "                  toolbox.max_depth,\n",
    "                  toolbox.min_child_weight,\n",
    "                  toolbox.gamma,\n",
    "                  toolbox.subsample,\n",
    "                  toolbox.colsample_bytree), n=1)\n",
    "\n",
    "# define the population to be a list of individuals\n",
    "toolbox.register('population', tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# the goal ('fitness') function to be maximized\n",
    "def eval(individual):\n",
    "    model, accuracy = classifier(dtrain=dtrain, params=individual)\n",
    "    return [accuracy]\n",
    "\n",
    "# the model for a specified individual\n",
    "def get_model(individual):\n",
    "    model, accuracy = classifier(dtrain=dtrain, params=individual)\n",
    "    return model\n",
    "\n",
    "# register the goal / fitness function\n",
    "toolbox.register('evaluate', eval)\n",
    "\n",
    "# register the crossover operator\n",
    "toolbox.register('mate', tools.cxTwoPoint)\n",
    "\n",
    "# register a mutation operator\n",
    "toolbox.register('mutate', tools.mutFlipBit, indpb=0.05)\n",
    "\n",
    "# operator for selecting individuals for breeding the next generation\n",
    "toolbox.register('select', tools.selTournament, tournsize=3)\n",
    "\n",
    "def main(pop_size=1, num_gen=1):\n",
    "    # random.seed(42)\n",
    "\n",
    "    # create an initial population\n",
    "    pop = toolbox.population(n=pop_size)\n",
    "\n",
    "    # CXPB is the probability with which two individuals are crossed\n",
    "    CXPB = 0.5\n",
    "     # MUTPB is the probability for mutating an individual\n",
    "    MUTPB = 0.2\n",
    "    print('===== Start of Genetic Algorithm =====')\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # Evaluate the entire population\n",
    "    fitnesses = list(map(toolbox.evaluate, pop))\n",
    "    for ind, fit in zip(pop, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    print('\\nEvaluated {} individual'.format(len(pop)))\n",
    "\n",
    "    # Extracting all the fitnesses of\n",
    "    fits = [ind.fitness.values[0] for ind in pop]\n",
    "\n",
    "    # Variable keeping track of the number of generations\n",
    "    g = 0\n",
    "    # Begin the evolution\n",
    "    while min(fits) > 0.6 and g < num_gen:\n",
    "        g = g + 1\n",
    "        print('Generation {}'.format(g))\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(pop, len(pop))\n",
    "        # Clone the selected individuals\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "        # Apply crossover and mutation on the offspring\n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "            # cross two individuals with probability CXPB\n",
    "            if random.random() < CXPB:\n",
    "                toolbox.mate(child1, child2)\n",
    "                # fitness values of the children\n",
    "                # must be recalculated later\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "\n",
    "        for mutant in offspring:\n",
    "            # mutate an individual with probability MUTPB\n",
    "            if random.random() < MUTPB:\n",
    "                toolbox.mutate(mutant)\n",
    "                del mutant.fitness.values\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        print('  Evaluated {} individuals'.format(len(invalid_ind)))\n",
    "\n",
    "        # The population is entirely replaced by the offspring\n",
    "        pop[:] = offspring\n",
    "\n",
    "        # Gather all the fitnesses in one list and print the stats\n",
    "        fits = [ind.fitness.values[0] for ind in pop]\n",
    "\n",
    "        length = len(pop)\n",
    "        mean = sum(fits) / length\n",
    "        sum2 = sum(x * x for x in fits)\n",
    "        std = abs(sum2 / length - mean ** 2) ** 0.5\n",
    "\n",
    "        print('  Min {}'.format(min(fits)))\n",
    "        print('  Max {}'.format(max(fits)))\n",
    "        print('  Avg {}'.format(mean))\n",
    "        print('  Std {}'.format(std))\n",
    "\n",
    "        best_ind = tools.selBest(pop, 1)[0]\n",
    "        print('Best individual so far is {}\\nBest fitness value {}'.format(best_ind, best_ind.fitness.values))\n",
    "\n",
    "    print('\\n====== End of evolution =====\\n')\n",
    "\n",
    "    best_ind = tools.selBest(pop, 1)[0]\n",
    "    print('Best individual is {}\\nBest fitness value {}'.format(best_ind, best_ind.fitness.values))\n",
    "\n",
    "    print('Get best model')\n",
    "    clf = get_model(best_ind)\n",
    "\n",
    "    dtest = xgb.DMatrix(data=X_test, label=None)\n",
    "\n",
    "    print('Start of prediction')\n",
    "    preds = clf.predict(dtest)\n",
    "    print('Prediction done!')\n",
    "\n",
    "    result = pd.DataFrame.from_dict({'acc_id': test['acc_id']})\n",
    "    result['label'] = pd.Series(pd.DataFrame(preds).idxmax(axis=1))\n",
    "    result['label'].replace({0: '2month', 1: 'month', 2: 'retained', 3: 'week'}, inplace=True)\n",
    "    now = datetime.datetime.now()\n",
    "    now = now.strftime('%Y-%m-%d_%H_%M')\n",
    "    filename = 'submission_'+now+'.csv'\n",
    "    result.to_csv(filename, index=False)\n",
    "    end_time = datetime.datetime.now()\n",
    "    total_time = (end_time.microsecond - start_time.microsecond)/1000\n",
    "    print(result.label.value_counts())\n",
    "    print('Finish time {} ms'.format(total_time))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pop_size = 10\n",
    "    num_gen = 100\n",
    "    main(pop_size=pop_size, num_gen=num_gen)\n",
    "    # read_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(params):\n",
    "    params = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 3,\n",
    "    'seed': 42,\n",
    "    'n_jobs': 6,\n",
    "    'silent': True,\n",
    "    'eta': params[0],\n",
    "    'n_estimators': params[1],\n",
    "    'max_depth': params[2],\n",
    "    }\n",
    "    \n",
    "    train_preds = np.zeros((len(X_train), len(np.unique(y_train))))\n",
    "    test_preds = np.zeros((len(X_test), len(np.unique(y_test))))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for i, (train_index, valid_index) in enumerate(skf.split(X_train, y_train)):\n",
    "        X_train_data, X_valid_data = X_train.iloc[train_index,:], X_train.iloc[valid_index,:]\n",
    "        y_train_data, y_valid_data = y_train.iloc[train_index,:], y_train.iloc[valid_index,:]\n",
    "\n",
    "        # Convert our data into XGBoost format\n",
    "        d_train = xgb.DMatrix(X_train_data, y_train_data)\n",
    "        d_valid = xgb.DMatrix(X_valid_data, y_valid_data)\n",
    "        d_test = xgb.DMatrix(X_test) # .values\n",
    "\n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "        clf = xgb.train(params, d_train, num_boost_round=10, evals=watchlist, early_stopping_rounds=5, verbose_eval=False)\n",
    "\n",
    "        y_pred = clf.predict(d_valid)\n",
    "        score = f1_score(y_valid_data.values.flatten(), np.argmax(y_pred, axis=1), labels=None, average=None)\n",
    "\n",
    "        train_preds[valid_index] = clf.predict(d_valid)\n",
    "        test_preds += clf.predict(d_test) / skf.n_splits\n",
    "        accuracy = f1_score(y_test.values.flatten(), np.argmax(test_preds, axis=1), average=None).mean()\n",
    "        \n",
    "    return clf, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3, random_state=42)\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(skf.split(X_train, y_train)):\n",
    "    X_train, X_valid = X_train[train_index], X_train[valid_index]\n",
    "    y_train, y_valid = y_train[train_index], y_train[valid_index]\n",
    "\n",
    "    # Convert our data into XGBoost format\n",
    "    d_train = xgb.DMatrix(X_train, y_train)\n",
    "    d_valid = xgb.DMatrix(X_valid, y_valid)\n",
    "    \n",
    "    d_test = xgb.DMatrix(X_test) # .values\n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "    # Train the model! We pass in a max of 1,600 rounds (with early stopping after 70)\n",
    "    # and the custom metric (maximize=True tells xgb that higher metric is better)\n",
    "    mdl = xgb.train(params, d_train, 1600, watchlist, early_stopping_rounds=20)\n",
    "\n",
    "    # Predict on our test data\n",
    "    p_test = mdl.predict(d_test, ntree_limit=mdl.best_ntree_limit)\n",
    "    sub += p_test / skf.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = ''\n",
    "for i in m:\n",
    "    st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st =''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((\"*\" * n + \"\\n\") * m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(x)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in count:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count.most_common(2)[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets = []\n",
    "# for line in open(r'C:/Users/unerue/Desktop/input/metadata.json', 'r'):\n",
    "#     tweets.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_json(r'C:/Users/unerue/Desktop/input/metadata.json', lines=True)\n",
    "magazine = pd.read_json(r'C:/Users/unerue/Desktop/input/magazine.json', lines=True)\n",
    "users = pd.read_json(r'C:/Users/unerue/Desktop/input/users.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:/Users/unerue/Desktop/input/metadata.json', compression='gzip', header=0, sep=' ', quotechar='\"', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(r'C:/Users/unerue/Desktop/input/predict/dev.users', header=None)\n",
    "pd.read_csv(r'C:/Users/unerue/Desktop/input/predict/test.users', header=None)\n",
    "pd.read_csv(r'C:/Users/unerue/Desktop/input/predict/dev.users', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PATH = r'C:/Users/unerue/Desktop/input/contents/'\n",
    "\n",
    "lines = []\n",
    "with os.scandir(PATH) as entries:\n",
    "    for entry in entries:\n",
    "        with open(entry) as f:\n",
    "            for line in f:\n",
    "                lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import pandas as pd\n",
    "with tarfile.open(r'C:/Users/unerue/Desktop/input/predict/.tar', 'r:*') as tar:\n",
    "    csv_path = tar.getnames()[0]\n",
    "    print(csv_path)\n",
    "    df = pd.read_csv(tar.extractfile(csv_path), header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
